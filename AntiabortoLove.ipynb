{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos los datos desde un csv\n",
    "df_tweet = pd.read_csv(\"data/Salvemos las dos vidas.csv\", sep = \"+\")\n",
    "palabrasqueno = pd.read_csv(\"data/palabras_eliminadas.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     aborto\n",
       "1                      https\n",
       "2         #salvemoslas2vidas\n",
       "3     #noalabortoenargentina\n",
       "4                          …\n",
       "5                          ¿\n",
       "6                          •\n",
       "7                 #rusia2018\n",
       "8                  #copa2018\n",
       "9              #todavidavale\n",
       "10       #argentinaesprovida\n",
       "11      #salvemoslasdosvidas\n",
       "12                #sialavida\n",
       "13    #abortosesionhistorica\n",
       "14                        rt\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabrasqueno['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se definen las stpowords  más las palabras que no queremos incluir\n",
    "stopwords = stopwords.words('spanish')+list(string.punctuation)+ list(palabrasqueno['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procesar tweet\n",
    "def procesar_tweet(text, tokenizer=TweetTokenizer(), stopwords=[]):\n",
    "    \n",
    "    '''Funcion para procesar Tweets que recibe un tweets, un objeto\n",
    "    que servirá para tokenizar los tweet y una lista de stopwords'''\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tok for tok in tokens if tok not in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "topicos = dict()\n",
    "i = 0\n",
    "for tweet in df_tweet.Text:\n",
    "    topicos[i] = procesar_tweet(tweet, tknzr, stopwords)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [x for x in topicos.values()]\n",
    "dictionary = corpora.Dictionary(lista)\n",
    "print (len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in lista]\n",
    "ldamodel = LdaModel(corpus = corpus, num_topics = 5, id2word = dictionary, iterations = 2000, passes = 10)\n",
    "ldamodel.print_topics (5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, mds=\"tsne\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
